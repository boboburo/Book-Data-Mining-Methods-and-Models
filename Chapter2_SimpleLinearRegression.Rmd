---
title: "Chapter 2 Regression Modelling Questions"
author: "Brian Carter"
date: "17 July 2015"
output:
  html_document:
    toc: true
    number_sections: true
---


# Clarifying the Concepts


Determine whether the following statements are true or false. If a statement is false,explain why and suggest how one might alter the statement to make it true.

**(a) The least-squares line is that line which minimizes the sum of the residuals.**

<span style="color:green"> TRUE: </span> The least-squares line minimizes the population sum of squared errors. Errors can be called residuals. $SSE_p= \displaystyle\sum_{i=1}^{n} {\varepsilon}_{i}^{2} = \displaystyle\sum_{i=1}^{n}(y_i - {\beta}_0 - {\beta}_1x_i)^2$. All recall that $SST = SSR + SSE$, therefore the least-squares line should maxmimise the sum of the regression (SSR), i.e. mops up the error. 

**(b) If all the residuals equal zero, SST = SSR.** <span style="color:green"> TRUE: </span>

**(c) If the value of the correlation coefficient is negative, this indicates that the variables are negatively correlated.** <span style="color:green"> TRUE: </span>

**(d) The value of the correlation coefficient can be calculated given the value of $r^2$ alone.**

<span style="color:green"> TRUE: </span>  $r^2$ is the **cooefficent of determiniation** in simple linear regression. It can be calculated by $\frac{SSR}{SST}$. r is the **Peasron product moment correlation coefficient.** Recall 

$$ \sigma^2 \backslash s^2 = Variance_(pop. \backslash samp.) = \frac{1}{N}\sum_{i=1}^{N} (x_i-\mu)^2 \backslash \frac{1}{N-1}\sum_{i=1}^{N} (x_i-\bar{x})^2$$

$$\sigma \backslash s = Standard Deviation = \sqrt{\sigma^2} \backslash \sqrt{s^2} $$

The standard deviation measures the variance at the same scale as the entries, rather than the square of the differences.The **Peasron product moment correlation coefficient.** is called rho \rho for a population or **r** for a sample. 

$$r = r_{xy} = \frac{1}{N-1} \sum_{i=1}^{N} \left( \frac{x_i-\bar{x}}{s_x} \right) \left( \frac{y_i-\bar{y}}{s_y} \right)$$ 

This is how many standard scores is the point away from the mean and do they follow a pattern, therefore no need to scale as each obeys its own scale. This Through a series of steps it is easy to show that 

$$ r = \pm \sqrt{r^2}.$$

It is + when $\beta_1$ (the estimate slope of the regression line) is + and - when $\beta_1$ is -.


**(e) Outliers are influential observations.**

That depends. Outliers have high residual value (error). However it may or may not be *influential*. Usually it would require that th point has *high leverage* score also. 

**(f) If the residual for an outlier is positive, we may say that the observed y-value is higher than the regression estimated, given the x-value.**  <span style="color:green"> TRUE: </span>

**(g) An observation may be influential even though it is neither an outlier nor a high leverage point.** <span style="color:red"> FALSE: </span> But it may be possible for a point not quite flagged as an outlier or higher leverage through a combination of the both become influential.

**(h) The bestway of determining whether an observation is influential is to see whether its Cook’s distance exceeds 1.0.**

Standardized residuals are residuals divided by the standard error, so that they are all on the same scale. 

$$residual \backslash error = y_i-\hat{y_i}$$

$$standardized \: reisdual = s_{i,resid} = s\sqrt{1-h_i} $$ 

s here is the **standard error of the estimate** not the sample standard deviation, although it is the same calculation bar the degrees of freedom. The MSE is similar to the variance, therefore standard error similar to standard deviation. Std. resiudals that are greater than 2 might be considered outliers - 2 standard deviations etc. 

$$s = \sqrt{MSE} = \sqrt{mean \; squared \; error} = \sqrt{\frac{SSE}{n-m-1}}$$

$$SSE = \sum_{i=1}^{N}  (y_i-\hat{y_i})^2$$

$h_i$ is the leverage. 

$$h_i = \frac{1}{n} + \frac{(x_i-\bar{x})^2}{\sum (x_i-\bar{x})^2}$$

Leverage focuses on the predictors. The further a point is away from the mean of a predictor the greater the leverage. A leverage value greater than $\frac{2(m+1)}{n}$ or $\frac{3(m+1)}{n}$ may be high leverage.

Cook's distance measures the level of infludence of an observation by taking into account boch the size of the residual an the amount of leverage for that observation. 

$$Cook's = D_i = \frac{(y_i-\hat{y})^2}{(m+1)s^2}\frac{h_i}{(1-h_i)^2}$$

Where s the standard errro of the estimate. Rule of thumb is Cook's greater than 1 many be influential, but also can use the F-distribution with $F_{(m,n-m)}$ degrees of freedom. If the value is in 25th percentile not influential, over the median it is. 

**(i) If one is interested in using regression analysis in a strictly descriptive manner, with no inference and no model building,one need not worry quite so much about assumption validation.**

Not true, if $\varepsilon$, does not have constant variance, regardless of x, (i.e. jumps in values, breaks) might not work. And other assumptions.  

**(j) In a normality plot, if the distribution is normal, the bulk of the points should fall on a straight line.**  <span style="color:green"> TRUE: </span>

These is also an Anderson-Darling test for normality. Small values of AD indciate normal distributions is a better fit. 

**(k) The chi-square distribution is left-skewed.** <span style="color:red"> FALSE: </span>

No a $\chi^2$ distribution is right skewed - (long tail on the right) 

```{r, echo=TRUE}

#Talk a little bit about where chi- comes frome
x <- rchisq(1000, 5)
hist(x, prob=TRUE)
curve( dchisq(x, df=5), col='green', add=TRUE)
curve( dchisq(x, df=8), col='blue', add=TRUE)
curve( dchisq(x, df=10), col='red', add=TRUE )


```

**(l) Small p-values for the Anderson–Darling test statistic indicate that the data are right-skewed.** <span style="color:red"> FALSE </span>

The null hyppothesis is that the normal distribution fits, so that small p-values will indiate lack of fit. There is also another test the Shapiro Wilks test. 

```{r, echo=FALSE}
#R has a quantile-quantil plot (QQ) . Plots data against a standard normal distibution  X = N(0,1) with mean 0 and sd 1. There is also a qqline which adds a line to the normal QQ plot. This line makes it easier to evaluate a clear deviationfrom normality. the licster all the points lie to the line the closer the distirbution of the samples comes from a noraml distirbution. 


x1 <- rchisq(1000, 5)   #Generate chi-sqaure distribution
x2 <- rnorm(250,10,1)   #Generate normal distribution with 250 points, mean 10, sd 1

qqnorm(x1, main='QQPlot Test for Normality',xlab="X")   #Plots the data against a normal distribution
qqline(x1,col="red" )   #Line where the it should lie

#Normal test
sw=stats::shapiro.test(x1)      #Shapiro-Wilks test
ks=stats::ks.test(x1,"pnorm",mean(x1),sqrt(var(x1))) # Kolmogorov-Smirnov test
ad=nortest::ad.test(x1) # Anderson-Darling test


# skewness and kurtosis, they should be around (0,3) for normal.


text(1.5,7, cex=.8, pos=4, paste(" mean =", round(mean(x1), digits=2),
															 "\n obs = ", length(x1) ,
															 "\n standard dev. =", round(sd(x1,na.rm=TRUE),digits=2),
															 "\n skewness =", round(e1071::skewness(x1), digits=2),
															 "\n kortosis =",round(e1071::kurtosis(x1),digits=2),
															 "\n AD =",round(ad$statistic,digits=2),
															 "p =", round(ad$p.value,digits=2),
															 "\n SW =",round(sw$statistic,digits=3),
															 "p =", round(sw$p.value,digits=3)))

op <- par(fig=c(.02,.5,.4,.98), new=TRUE)
#A numerical vector of the form c(x1, x2, y1, y2) which gives the (NDC) coordinates of the figure region in the display region of the device. If you set this, unlike S, you start a new plot, so to add to an existing plot use new = TRUE as well.		 

#Create the chart and put it in the original figure inside the box
hist(x, probability=T,col="light blue", xlab="", ylab="", main="", axes=F)
lines(density(x), col="red", lwd=2)
box()
par(op)
```


**(m) A funnel pattern in the plot of residuals versus fits indicates a violation of the independence assumption.**
<span style="color:green"> TRUE </span>. It indicates the the residuals do not have constant-variance and get larger according to some 
underlying rule. 

**2.2. Describe the difference between the estimated regression line and the true regression line.**

The true regression line is not usually known. A estimate of the true regression line by calculated the coefficients, based on the observed data. The estimated coefficents are calculated using least squares method. The estimate regression line is then calculated using the coefficents $\beta_0$ and $\beta_1$. This is sometimes called the fitted lined. The difference between the true value and the estimated value is the residual, $e_i = y_i - \hat{y_i}$
``

**2.3.Calculate the estimated regression equation for the orienteering example using the data in Table 2.3. Use either the formulas or software of your choice.**
```{r, echo=FALSE}
data<-read.csv(file="Data/orienteer.csv",header=T)
#nice ggplot 
library(ggplot2)
c <-ggplot(data, aes(Time, Distance),label=Student)
c+geom_point()+geom_text(aes(label=Subject),hjust=0, vjust=0)+geom_smooth(method="lm", size=0.5, se=T)
```

```{r, echo=FALSE}
fit <- lm(Distance ~ Time,data=data)
opar <- par(mfrow = c(2,2), oma = c(0, 0, 1.1, 0))
plot(fit, las = 1)
par(opar)
```

From these plots, we can identify observations 7, 10, 8 as possibly problematic to our model. We can look at these observations to see which states they represent.


```{r kable1,echo=FALSE}
library(knitr)
kable(summary(fit)$coef, digits=3,format = "markdown")
```

```{r pander1,echo=FALSE}
library(pander)
panderOptions("digits", 3)
pander(anova(fit),caption="ANOVA Table of lm fit - Analysis of Variance")
```


```{r,echo=FALSE}
time=data$Time
y=data$Distance
yhat=predict(fit,data=data)
yhat=fit$fitted.values
resids<-fit$residuals
rstand <- rstandard(fit)

#leverage
leverage<-hatvalues(fit)
fitInfluence<-influence(fit)
#leverage==fitInfluence$hat
#sum(leverage) #adds up to the number of parameters in the model. 

d1 <- cooks.distance(fit)
d1.f.test<-pf(d1,df1=1,df2=nrow(data)-1)
datapredict <- cbind(data$Time,y,yhat,resids,rstand,leverage,d1,d1.f.test)
#datapredict[order(-abs(d1.f.test)),]
```

```{r pander}
library(pander)
panderOptions("digits",2)
pander(summary(fit))
```

```{r,results='asis'}
#knitr::kable(datapredict[order(-abs(d1.f.test)),])
```


```{r, echo=FALSE}
datapredict <- cbind(time,y,yhat,resids,rstand,leverage,d1,d1.f.test)
panderOptions('table.alignment.default','left')
pander(datapredict[order(-abs(d1.f.test)),],caption="Sorted by abs(d1.f.test)",split.tables=150)
#table.split=Inf,split.cells = c(5,5,5,5,5,5,5,5,5,5
```


```{r, eval=FALSE,echo=FALSE}
#Aside on general probability functions in R, based on normal distribution
dnorm(0.5,0,1,log=F) #Two tailed density  NEED TO DOUBLE CHECK THIS WHAT IT IS
pnorm(0.5)  #area to the left of the curve
qnorm(0.691425) #area to value
rnorm(3) #generate random numbers or random deviates. 
#Convention followed for all distributions.
```

2.4. 
**Where would a data point be situated which has the smallest possible leverage?**
A point with the smallest leverage would be the value nearest the mean of value of the predictor in question. In this case time has $\bar{x}$ = 6. There is one point, 7 that is exactly this there its leverage is simply 1/n = 0.0909... 

**2.5. Calculate the values for leverage, standardized residual, and Cook’s distance for the hard-core hiker example in the text.** 
Hard-core hiker in this example Time is 16 and distance is 39. 

**2.6. Calculate the values for leverage, standardized residual, and Cook’s distance for the eleventh hiker who had hiked for 10 hours and traveled 23 kilometers. Show that although it is neither an outlier nor of high leverage, it is nevertheless influential**

**2.7. Match each of the following regression terms with its definition.**
```{r,echo=FALSE}

#Matching phrase to meaning. 
terms<-c("Influential observation",
         "SSE","r-sq.",
         "Residual",
         "s",
         "High leverage point","r","SST","Outlier","SSR","Cooks distance")

defs<-c("An observation that alters the regression parameters significantly based on its prsence or absence in the data set.",
        "Represents an overall measure of the error in prediction resulting from the use of the estimated regression equation.",
        "The proportion of the variability in the response that is explained by the linear relationship betwen the predictor and response variables.",
        "The vertical distance between the repsonse predicted an the actual response.",
        "Measures the **typical** difference between the predicted and actual response values. - standarzied.",
        "An observation that is extreme in the predictor space (x-variables), without reference to the response variable.",
        "Measures the strength of the linear relationship between two quantitative variables, with values ranging form -1 to 1",
        "Represents the total variability in the values of the response variable  alone, without referecne to the predictor.",
        "An observation that has a very large standardized residual in absolute value.",
        "Measures the overall improvement in predictor accuracy when usingthe regression as opposed to ignoring the predictor information",
        "Measures the level of influence of an observation by taking into account both the size of the residual and the amount of leverage for that observation.") 
```

```{r,echo=FALSE}
df<-data.frame(Term=terms,Definition=defs)
df$Term<-as.character(df$Term) #go in as factors. 

library(pander)
panderOptions('table.alignment.default','left')
pander(df,caption="Matching", split.cells = c(10,60), split.tables =150)
```

**2.8. Explain in your own words the implications of the regression assumptions for the behavior of the response variable y. **

The residuals should form a normal distrbution, with a mean. Deviation from this means that the assumptions have been invalidated somehow. The predictions should be contant, no large jumps. The predicted value is the mean of a normal distibution around a normal distirbution with constat variance $\sigma^{2}$ regardless of what value of th predictor. 


**2.9. Explain what statistics from Table 2.11 indicate to us that there may indeed be a linear relationship between x and y in this example, even though the value for $r^{2}$ is less than 1%.**

The coefficent of determination $r^2$ is less than 1%. This can be also calculated by $\frac{SSR}{SST}$. The value of $r^2$ can be between 0 and 1. It is way of describing the relation between two variables and can also be found by squaring the correlation coeffience (r Pearson's etc.). 

For simple linear regression the t-test and the F-test are equivalent. 

A hypothesis test are too blunt to determine if a predictor is signifcant. There a *t-interval* confidence interval may be calculated. For 95% confidence interval; $b_1=0.5594$. Not given the number of points, but if we were we could calculate $$\beta_1 \pm (t_{n-2)({S_{b1}}})$$.

For 75 observations this is, $0.05594 \pm (2.0)(.03056) = (-0.55526,0.66714)$. As $\beta_1$ crosses zero we can't be sure the predictor is of benefit. 


**2.10. Which values of the slope parameter indicate that no linear relationship exists between the predictor and response variables? Explain how this works.**

If the slope parameters is 0 or very close to 0 it means that there is no linear relationship between the predictor and response variables. 


**2.11. Explain what information is conveyed by the value of the standard error of the slope estimate.**

s or the *standard error of the estimate*. It is calculated by the $\sqrt{MSE}$ which is $$\frac{SSE}{n-m-1}$$. The SSE is above, it is the sum of **squared** errors.  Like a standard deviation, it is the stand residual or error, it therefore can be used to cosider the precision of the prediction. Smaller values are better.


**2.12. Describe the criterion for rejecting the null hypothesis when using the p-value method for hypothesis testing. Who chooses the value of the level of significance, α? Make up a situation (one p-value and two different values of α) where the very same data could lead to two different conclusions of the hypothesis test. Comment.**


**2.13. (a) Explain why an analyst may prefer a confidence interval to a hypothesis test. (b) Describe how a confidence interval may be used to assess significance. **

**2.14. Explain the difference between a confidence interval and a prediction interval. Which interval is always wider? Why? Which interval is probably, depending on the situation, more useful to a data miner? Why? **


**2.15. Clearly explain the correspondence between an original scatter plot of the data and a plot of the residuals versus fitted values.**


**2.16. What recourse do we have if the residual analysis indicates that the regression assumptions have been violated? Describe three different rules, heuristics, or family of functions that will help us.**


**2.17. A colleague would like to use linear regression to predict whether or not customers will make a purchase based on some predictor variable. What would you explain to your colleague? **


# WORKING WITH DATA


**2.18. Based on the scatter plot of attendance at football games versus winning percentage of the home team shown in Figure E2.18, answer the following questions. **

(a) Describe any correlation between the variables, and estimate the value of the
correlation coefficient r . - Answer: the variables *Winning Percent* and *Attendence* are positively correlated. Approximately around 0.8. (From regressiion $R^{2}=0.747 \Rightarrow r = \sqrt{0.747}=0.864$)

(b) Estimate as best you can the values of the regression coefficients b0 and b1. - Answer: $b_0$ is around 10500. For $b_1$ every 10 units of *Winning Percent* is 1000 units of *Attendence* So about 100 or 80 given correlation. True figure is 77.22

(c) Will the p-value for the hypothesis test for the existence of a linear relationship
between the variables be small or large? Explain. The p-value for the existence of the linear realtionship will be small, as there is a linear relationship. (Explain more)

(d) Will the confidence interval for the slope parameter include zero? Explain. No it will not include zero as a positive, strong relationship exists. 

(e) Will the value of s be closer to 10, 100, 1000, or 10,000? Why? The value of s will be closer to 1000 as it is the standard error and the change in units is in the 1000s

(f) Is there an observation that may look as though it is an outlier? The highest value for *Attendence* could possibly be an outlier as it is high, but lower than others for values of *Winning Percent*. 


**2.19. Use the regression output (shown in Table E2.19) to verify your responses from Exercise 2.18.**


**2.20. Based on the scatter plot shown in Figure E2.20, answer the following questions.**

(a) Is it appropriate to perform linear regression? Why or why not? It is probably not correct to perform linear regression as a lot of the points are squished to the left - right skewed distribution. Therfore does not follow the assumpstions of normality. 

(b) What type of transformation or transformations are called for? Use the bulging
rule. Using the bulging heuritis it would be x - down, y up. Therfore potentially $\sqrt{x}$ or $ln(x)$ and $y^{2}$ or $y^{3}$ to achieve normality.  


**2.21. Based on the regression output shown in Table E2.21 (from the churn data set), answer the following questions.**

(a) Is there evidence of a linear relationship between z vmail messages (z-scores of
the number of voice mail messages) and z day calls (z-scores of the number of
day calls made)? Explain.

(b) Since it has been standardized, the response z vmail messages has a standard
deviation of 1.0. What would be the typical error in predicting z vmail messages
if we simply used the sample mean response and no information about day calls?
Now, from the printout, what is the typical error in predicting z vmail messages
given z day calls? Comment.

# Hands on Analysis


**2.22. Open the baseball data set, which is available at the book series Web site. Subset the data so that we are working with batters who have at least 100 at bats.**

(a) We are interested in investigating whether there is a linear relationship between
the number of times a player has been caught stealing and the number of stolen
bases the player has. Construct a scatter plot with caught as the response. Is there
evidence of a linear relationship?


```{r,warning=FALSE,messages=FALSE}
baseball<-read.csv(file="Data/baseball.csv",header=T) #nrow 331
sub_bb<-baseball[baseball$at_bats >= 100,]  #nrow 209

library(ggplot2)
p1<-ggplot(sub_bb,aes(x=stolen_bases,y=caught_stealing))
p1<-p1+geom_point(aes(color=triples))
p1<-p1+labs(x="Stolen Bases",y="Caught Stealing")
p1<-p1+theme(legend.position=c(1,0),legend.justification=c(0.9,-0.1)) #not sure how this is working
p1
```

(b) Based on the scatter plot, is a transformation to linearity called for? Why or why
not? Difficult to tell potentiall could do this. Lets try and plot. Seems right skewed. x down and y up. 

```{r,warning=FALSE,messages=FALSE}
p2<-ggplot(sub_bb,aes(x=sqrt(stolen_bases),y=sqrt(caught_stealing))) #Tranform according to bulging heuristic
p2<-p2+geom_point(aes(color=triples))
p2<-p2+labs(x="Sqrt. Stolen Bases",y="Sq. of Caught Stealing")
p2<-p2+theme(legend.position=c(1,0),legend.justification=c(0.9,-0.1)) 
p2
```

Not entirely clear if this improves anything. Might look at applying a Box-Cox transformation in future. 


(c) Perform the regression of the number of times a player has been caught stealing
versus the number of stolen bases the player has.

```{r}
fit<-lm(caught_stealing~stolen_bases,data=sub_bb)
fit.anova<-anova(fit)
library(pander)
pander(summary(fit))
pander(fit.anova)

```

(d) Find and interpret the statistic which tells you how well the data fit the model.
The value of *coefficent of determination* $R^2$ is 0.49, which means about half the variance in response is accounted for. Remember $R^2$ equals $\frac{SSR}{SST}$ = $\frac{839}{1710}$. 


(e) What is the typical error in predicting the number of times a player is caught
stealing given his number of stolen bases?
The standard error $s=2.1$

(f) Interpret the y-intercept. Does this make sense? Why or why not?

The y-intercept is where x equal zero. x is *stolen_bases* which can equal zero. However looking at the mean where this is the case its not clear that a value of 1.1 is correct. It would seem to large. 


```{r}
mean(sub_bb$caught_stealing[sub_bb$stolen_bases==0])
#0.666667
mean(sub_bb$caught_stealing[sub_bb$stolen_bases==0 & sub_bb$caught_stealing >0])
#1.764706
```


(g) Inferentially, is there a significant relationship between the two variables? What
tells you this? The small value of the significant test in the p-value. 

(h) Calculate and interpret the correlation coefficient. The correlation coefficent is 0.70 meaning there is midly strong positive correlation. 

(i) Clearly interpret the meaning of the slope coefficient. The slope coefficent is 0.27. so for every 1 unit in crease in *stolen bases* the *caught stealing* will increase by 0.27 present.

(j) Suppose someone said that knowing the number of stolen bases a player has
explains most of the variability in the number of times the player gets caught
stealing. What would you say? It only explains about half the variability. 

**2.23. Open the cereals data set, which is available at the book series Web site.**

(a) We are interested in predicting nutrition rating based on sodium content. Construct
the appropriate scatter plot.

```{r,warning=FALSE,message=FALSE,echo=FALSE}

#Add some more control with theme, scale_color_manual, 

cereals<-read.csv("Data/cereals.csv",header=T)
library(ggplot2)
p1<-ggplot(cereals,aes(x=sodium,y=rating))
p1<-p1+geom_jitter(position=position_jitter(width=2.5,height=2.5),aes(color=as.factor(type)))
p1<-p1+labs(x="Sodium",y="Rating",title="Little bit of Jitter")
p1<-p1+theme(legend.position=c(1,0),legend.justification=c(0.9,-0.1)) #not sure how this is working
p1<-p1+scale_color_manual("Legend Title\n",labels = c("C", "H"),values=c("blue","red"))
p1+ theme(axis.text.x=element_text(size=16,face="bold"), axis.title.x=element_text(size=22,color="red"),
        axis.text.y=element_text(size=16,face="bold"), axis.title.y=element_text(size=22,color="blue"),
        plot.title=element_text(size=20, face="bold", color="darkgreen"))


```

```{r,warning=FALSE,message=FALSE,echo=FALSE}
cereals<-read.csv("Data/cereals.csv",header=T)
library(ggplot2)
p1<-ggplot(cereals,aes(x=sodium,y=rating))
p1<-p1+geom_jitter(position=position_jitter(width=2.5,height=2.5),aes(color=as.factor(type)))
p1<-p1+labs(x="Sodium",y="Rating",title="Little bit of Jitter")
p1<-p1+theme(legend.position=c(1,0),legend.justification=c(0.9,-0.1)) #not sure how this is working
p1<-p1+scale_color_manual("Legend Title\n",labels = c("C", "H"),values=c("blue","red"))
p1+ theme(axis.text.x=element_text(size=16,face="bold"), axis.title.x=element_text(size=22,color="red"),
        axis.text.y=element_text(size=16,face="bold"), axis.title.y=element_text(size=22,color="blue"),
        plot.title=element_text(size=20, face="bold", color="darkgreen"))
```




(b) Based on the scatter plot, is there strong evidence of a linear relationship between
the variables? Discuss. Characterize their relationship, if any. There is evidence of some linear relationship, moderate relationship.

(c) Perform the appropriate regression.

```{r}
fit<-lm(rating~sodium, cereals)
library(pander)
pander(summary(fit))

fit.anova<-anova(fit)
pander(fit.anova)

#Similar way of construction the linear regression with gineral linear model. 
fit2<-glm(rating~sodium,data=cereals,family=gaussian(link="identity"))

```

(d) Which cereal is an outlier? Explain why this cereal is an outlier. 

Construct a visual plot to examine the Standardized reisdual an the Fitted Value. The point **4** is an outlier as it is more than twice the standard residual. 3 and 1 are returned also in the the *plot.lm* results. 

```{r,warning=FALSE,message=FALSE,echo=FALSE}
#Create a plot of the residuals against fitted values. 
r<-fit$residuals
yh<-predict(fit)

#Any standardized residual that is greater than 1.9 consider as an outlier and output its row.name
#Remember the standardized residual is the total square error of the regression divided by df. 
#Residual standard error:
rse<-sqrt(deviance(fit)/df.residual(fit))
#Alternatively
rs<-rstandard(fit)
names<-row.names(cereals)

#Create the plot 
d<-data.frame(r,yh)

p4<-ggplot(d,aes(x=yh,y=r),label=names)+
	geom_point(size=2)+
	labs(x="Fitted Values",y="Residuals",title="Fitted values vs. Residuals")+
	geom_text(aes(label=ifelse(abs(rs)>1.8,as.character(names),'')),hjust=0.5,vjust=-1,color="red")+
	geom_hline(yintercept=rse,color="blue",linetype="dashed")+
	geom_hline(yintercept=0)+
	geom_hline(yintercept=-rse,color="blue",linetype="dashed")+
  annotate("text", label = paste("Residual Std. Error",round(rse,2)),size=3, x = 32, y = rse+1.5,hjust=-0.05)+
	annotate("text", label = paste("Residual Std. Error",round(rse,2)),size=3, x = 32, y = -(rse+1.5),hjust=-0.05)+
	geom_smooth(method="loess",color="red")+   #default method is loess
	annotate("text",label = "The plot of the Fitted Values against the residuals.\nOutliers indicated as being greater than 1.8 * Residual St. Error.\nNormally this is set to 2 times the RSE.\nThe red line is the smoothed curve that passes through the residuals. \nThe flatter the better and closer to zero is better.",size=3,x=32,y=43,hjust=0)+
	geom_vline(xintercept=fit$coefficients[1],color="green")+
	annotate("text", x=53, y=35,size=3, label="Y-intercept = 53.40",angle = 90)
p4+ylim(-22,52)

#Alternatively could have done
plot(fit,which=c(1))
```

Now look at combining some plots, the predictor and the target with multiple plots. 

```{r,warning=FALSE,message=FALSE}
#Create a histogram of sodium but reverse the order using negation. 
pc2<-ggplot(cereals,aes(x=-sodium))+geom_histogram()
pc1<-p4+ylim(-25,55)

#markdown having problems with unit
#pc1<-p1+theme(plot.margin=unit(c(1,1,-0.5,1), "cm"))
#pc2<-p2+theme(plot.margin=unit(c(1,1,-0.5,1), "cm"))

pc2<-pc2+theme(axis.text.y=element_blank(),axis.ticks.y=element_blank())
library(gridExtra)
grid.arrange(pc2,pc1,ncol=1,nrow=2,heights=c(1,4))
```

(e) What is the typical error in predicting rating based on sodium content? The typical error is `r 
sqrt(deviance(fit)/df.residual(fit))` As shown by the dotted blue lines in the above graph. 



(f) Interpret the y-intercept. Does this make any sense? Why or why not? The y-intercept is `r fit$coefficients[1]`. This is the expect value when $b_1$ is zero. This could be a possible credible value. The mean when *sodium* is zero is `r mean(cereals$rating[cereals$sodium==0])`. The actual range of values is `r cereals$rating[cereals$sodium==0]`. So it is probably a bit on the low side. But $R^2$ is only .49 so this is expected. 

TO FINISH
(g) Inferentially, is there a significant relationship between the two variables? What tells you this? The 95% confidence interval of the slope is 

(h) Calculate and interpret the correlation coefficient.

```{r}

cor1<-cor(cereals$rating,cereals$sodium)
sse<-sum((cereals$rating-fit$fitted.values)^2)  #SSE
sst<-sum((cereals$rating-mean(cereals$rating))^2)
ssr<-sst-sse
cor2<-sqrt(ssr/sst)

```

The correlation co-efficent can be calculated as $$r=\sqrt{R^2}$$ which equals `r sqrt(ssr/sst)` or simply `r cor(cereals$rating,cereals$sodium)`


(i) Clearly interpret the meaning of the slope coefficient. The slope coefficient is `r fit$coefficients[2]` 

(j) Construct and interpret a 95% confidence interval for the true nutrition rating for all cereals with a sodium content of 100.

TO FINISH


(k) Construct and interpret a 95% confidence interval for the nutrition rating for a
randomly chosen cereal with sodium content of 100.

TO FINISH
**2.24. Open the California data set, which is available at the book series Web site.**

(a) Recapitulate the analysis performed within the chapter.

(b) Set aside the military outliers and proceed with the analysis with the remaining
848 records. Apply whatever data transformations are necessary to construct your
best regression model.



